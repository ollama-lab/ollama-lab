<script lang="ts">
  import type { ModelEntry, ModelInfo } from "$lib/models/models"
    import { IconDotsVertical } from "@tabler/icons-svelte";
    import convert from "convert";

  export let entry: ModelEntry

  let modelInfo: ModelInfo = {
    "modelfile": "# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llava:latest\n\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\nTEMPLATE \"\"\"{{ .System }}\nUSER: {{ .Prompt }}\nASSISTANT: \"\"\"\nPARAMETER num_ctx 4096\nPARAMETER stop \"\u003c/s\u003e\"\nPARAMETER stop \"USER:\"\nPARAMETER stop \"ASSISTANT:\"",
    "parameters": "num_keep                       24\nstop                           \"<|start_header_id|>\"\nstop                           \"<|end_header_id|>\"\nstop                           \"<|eot_id|>\"",
    "template": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
    "details": {
      "parent_model": "",
      "format": "gguf",
      "family": "llama",
      "families": [
        "llama"
      ],
      "parameter_size": "8.0B",
      "quantization_level": "Q4_0"
    },
    "model_info": {
      "general.architecture": "llama",
      "general.file_type": 2,
      "general.parameter_count": 8030261248,
      "general.quantization_version": 2,
      "llama.attention.head_count": 32,
      "llama.attention.head_count_kv": 8,
      "llama.attention.layer_norm_rms_epsilon": 0.00001,
      "llama.block_count": 32,
      "llama.context_length": 8192,
      "llama.embedding_length": 4096,
      "llama.feed_forward_length": 14336,
      "llama.rope.dimension_count": 128,
      "llama.rope.freq_base": 500000,
      "llama.vocab_size": 128256,
      "tokenizer.ggml.bos_token_id": 128000,
      "tokenizer.ggml.eos_token_id": 128009,
      "tokenizer.ggml.merges": [],            // populates if `verbose=true`
      "tokenizer.ggml.model": "gpt2",
      "tokenizer.ggml.pre": "llama-bpe",
      "tokenizer.ggml.token_type": [],        // populates if `verbose=true`
      "tokenizer.ggml.tokens": []             // populates if `verbose=true`
    }
  }
</script>

<div class="w-full h-dvh flex flex-col">
  <div class="flex flex-col px-4 py-2 lg:px-8 lg:py-6 overflow-y-scroll">
    <div class="flex flex-col md:px-8 md:py-10 lg:px-12 lg:py-16 gap-5">
      <div class="flex-auto flex">
        <h1 class="font-bold text-2xl flex-auto">{entry.name}</h1>
        <div class="flex place-items-center">
          <button class="btn-icon btn-icon-sm variant-ghost">
            <IconDotsVertical />
          </button>
        </div>
      </div>
      <div class="flex flex-col gap-1">
        <span>Model family: {modelInfo.details.family}</span>
        <span>Parameter size: {modelInfo.details.parameter_size}</span>
        <span>Model size: {convert(entry.size, "byte").to("best", "imperial").toString(2)} ({entry.size.toLocaleString()} bytes)</span>
      </div>
    </div>
  </div>
</div>
